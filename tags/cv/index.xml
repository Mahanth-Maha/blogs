<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>CV on Mahanth Yalla</title><link>https://mahanthyalla.in/blogs/tags/cv/</link><description>Recent content in CV on Mahanth Yalla</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 01 Jul 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://mahanthyalla.in/blogs/tags/cv/index.xml" rel="self" type="application/rss+xml"/><item><title>Pothole Detection UG Project</title><link>https://mahanthyalla.in/blogs/p/pothole-detection-ug-project/</link><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate><guid>https://mahanthyalla.in/blogs/p/pothole-detection-ug-project/</guid><description>&lt;h1 id="phd_fyp">&lt;a href="#phd_fyp" class="header-anchor">&lt;/a>PHD_FYP
&lt;/h1>&lt;p>A web app for The Pot Hole Detection on images and open CV and TensorFlow.&lt;/p>
&lt;p>The web app would include a way to upload the image that the user took of the pothole at certain location and our model would predict if there exist any pothole and then updates the database. So, that whenever Other person logs into the webpage, he will be notified that there is a pothole day by showing the marker over the location that the picture was taken.&lt;/p>
&lt;p>By using this markers on the location one could easily find out that the road that they are taking Contains a pothole, so They can slow down the vehicle or if they really want to avoid such potholes and don&amp;rsquo;t, Make any damage to the vehicle, They could take another path which has the less potholes.&lt;/p>
&lt;h2 id="testing-custom-images">&lt;a href="#testing-custom-images" class="header-anchor">&lt;/a>Testing custom images
&lt;/h2>&lt;p>extract &lt;a class="link" href="https://1drv.ms/u/s!AhCzSwMWU4mgjWNg2IixRGDqSpdw?e=zSWaGJ" target="_blank" rel="noopener"
>the zip file&lt;/a> into &lt;code> training_demo\&lt;/code> folder &amp;amp; then run&lt;/p>
&lt;p>extract &amp;rsquo;exported_models&amp;rsquo; into &lt;code> training_demo\&lt;/code> folder. (be careful while extracting, it should not create &amp;rsquo;exported_models&amp;rsquo; folder 2 times)&lt;/p>
&lt;p>install python-3.9.X ( prefer 3.9.12 )
go to the root folder of the project and run the command below (it is recommended to use a venv for testing the project)&lt;/p>
&lt;h2 id="anaconda">&lt;a href="#anaconda" class="header-anchor">&lt;/a>Anaconda
&lt;/h2>&lt;p>install the anaconda to make the env creation easy &lt;a class="link" href="https://www.anaconda.com/" target="_blank" rel="noopener"
>Download Here!&lt;/a>&lt;/p>
&lt;h3 id="setting-up">&lt;a href="#setting-up" class="header-anchor">&lt;/a>setting up
&lt;/h3>&lt;p>open the Anaconda Command Prompt or Anaconda Powershell Prompt after installtion&lt;/p>
&lt;p>Navigate to project dir&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">cd &amp;lt;PATH&amp;gt;\PHD_FYP
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="creating-v-env-to-run-project">&lt;a href="#creating-v-env-to-run-project" class="header-anchor">&lt;/a>Creating v-env to run project
&lt;/h3>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">conda create --name phd_fyp python==3.9.12
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">conda activate phd_fyp
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="note--possible-error-while-running-the-app">&lt;a href="#note--possible-error-while-running-the-app" class="header-anchor">&lt;/a>NOTE : POSSIBLE ERROR while running the app
&lt;/h3>&lt;p>The tensorflow usual throws an error as tf.gfile.GFile not found or tf has no attribute named gfile.
check reslving techniques here in&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;a class="link" href="https://stackoverflow.com/questions/55591437/attributeerror-module-tensorflow-has-no-attribute-gfile#:~:text=33-,in%202.0%2C%20tf.gfile.*%20is%20replaced%20by%20tf.io.gfile.*.,-when%20I%20get" target="_blank" rel="noopener"
>StackOverflow&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a class="link" href="https://github.com/tensorflow/tensorflow/issues/31315#:~:text=i%20solved%20the%20error%20by%20replacing%20tf.gfile.fastgfile%20to%20tf.io.gfile.gfile." target="_blank" rel="noopener"
>Tensorflow Issues&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>simply , &lt;em>&lt;strong>Replace tf.gfile.GFile to tf.io.gfile.GFile at line number 137&lt;/strong>&lt;/em>&lt;/p>
&lt;h2 id="end-user-product">&lt;a href="#end-user-product" class="header-anchor">&lt;/a>End User Product
&lt;/h2>&lt;h3 id="setup">&lt;a href="#setup" class="header-anchor">&lt;/a>Setup
&lt;/h3>&lt;p>the setup file is also included with requirements.txt, but &lt;em>I have wrote the script&lt;/em> in such a way that it ensures all the dependencies are installed on FIRST TIME RUN , ALL AT ONCE and no more installing or configuring is required except for the mentioned above&lt;/p>
&lt;p>Note : if one wants to delete entire database run &lt;code>python main.py&lt;/code>, which deletes all the entries till now.&lt;/p>
&lt;h2 id="web-app">&lt;a href="#web-app" class="header-anchor">&lt;/a>Web App
&lt;/h2>&lt;p>make sure you are in root directory i.e, &lt;code>PHD_FYP&lt;/code> and extracted the zip file into &lt;code> training_demo\&lt;/code> so that the structure will be &lt;code> training_demo\exported-models\models\&lt;/code> (be careful since it might be the case that it extracts &lt;code> training_demo\exported-models\exported-models\models\&lt;/code> if it&amp;rsquo;s a windows OS)&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">python app.py
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>go to the web link it provides probably &lt;a class="link" href="http://127.0.0.1:8080/" target="_blank" rel="noopener"
>http://127.0.0.1:8080/&lt;/a> , or better open which flask provides dynamically&lt;/p>
&lt;h3 id="web-app-interface-images">&lt;a href="#web-app-interface-images" class="header-anchor">&lt;/a>web app interface images
&lt;/h3>&lt;h4 id="running-app">&lt;a href="#running-app" class="header-anchor">&lt;/a>Running app
&lt;/h4>&lt;p>&lt;img src="https://mahanthyalla.in/blogs/DEMO_WEBAPP_run.png"
loading="lazy"
alt="DEMO_WEBAPP_run.png"
>&lt;/p>
&lt;h4 id="website-walk-through">&lt;a href="#website-walk-through" class="header-anchor">&lt;/a>website walk through
&lt;/h4>&lt;p>&lt;img src="https://mahanthyalla.in/blogs/DEMO_WEBAPP_1.png"
loading="lazy"
alt="DEMO_WEBAPP_1.png"
>
&lt;img src="https://mahanthyalla.in/blogs/DEMO_WEBAPP_2.png"
loading="lazy"
alt="DEMO_WEBAPP_2.png"
>
&lt;img src="https://mahanthyalla.in/blogs/DEMO_WEBAPP_3.png"
loading="lazy"
alt="DEMO_WEBAPP_3.png"
>&lt;/p>
&lt;h4 id="results---web-app">&lt;a href="#results---web-app" class="header-anchor">&lt;/a>Results - WEB app
&lt;/h4>&lt;p>&lt;img src="https://mahanthyalla.in/blogs/DEMO_WEBAPP_4.png"
loading="lazy"
alt="DEMO_WEBAPP_4.png"
>&lt;/p>
&lt;h2 id="running-app-in-mobile">&lt;a href="#running-app-in-mobile" class="header-anchor">&lt;/a>Running app in mobile
&lt;/h2>&lt;h4 id="website-walk-through-in-mobile">&lt;a href="#website-walk-through-in-mobile" class="header-anchor">&lt;/a>website walk through in mobile
&lt;/h4>&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: center">Demo&lt;/th>
&lt;th style="text-align: center">Images&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: center">&lt;img src="https://mahanthyalla.in/blogs/DEMO_MOBILE_WEB_APP_1.jpg"
loading="lazy"
alt="DEMO_MOBILE_WEB_APP_1.jpg"
>&lt;/td>
&lt;td style="text-align: center">&lt;img src="https://mahanthyalla.in/blogs/DEMO_MOBILE_WEB_APP_3.jpg"
loading="lazy"
alt="DEMO_MOBILE_WEB_APP_3.jpg"
>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: center">&lt;img src="https://mahanthyalla.in/blogs/DEMO_MOBILE_WEB_APP_2.jpg"
loading="lazy"
alt="DEMO_MOBILE_WEB_APP_2.jpg"
>&lt;/td>
&lt;td style="text-align: center">&lt;img src="https://mahanthyalla.in/blogs/DEMO_MOBILE_WEB_APP_4.jpg"
loading="lazy"
alt="DEMO_MOBILE_WEB_APP_4.jpg"
>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="results-and-maps">&lt;a href="#results-and-maps" class="header-anchor">&lt;/a>Results and maps
&lt;/h4>&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: center">WEB app in mobile&lt;/th>
&lt;th style="text-align: center">MAPS view in mobile&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: center">&lt;img src="https://mahanthyalla.in/blogs/DEMO_MOBILE_WEB_APP_5.jpg"
loading="lazy"
alt="DEMO_MOBILE_WEB_APP_5.jpg"
>&lt;/td>
&lt;td style="text-align: center">&lt;img src="https://mahanthyalla.in/blogs/DEMO_MOBILE_WEB_APP_6.jpg"
loading="lazy"
alt="DEMO_MOBILE_WEB_APP_6.jpg"
>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="running-as-a-product-for-an-image-cli">&lt;a href="#running-as-a-product-for-an-image-cli" class="header-anchor">&lt;/a>Running as a product for an image (CLI)
&lt;/h2>&lt;p>By running this python script it opens the Input image and the output image generated in separate window.&lt;/p>
&lt;p>Note : Replace the &amp;lt;image_path&amp;gt; with the actual image path while running the Script.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">python Main_CLI.py &amp;lt;image_path&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="cli-app-interface-images">&lt;a href="#cli-app-interface-images" class="header-anchor">&lt;/a>CLI app interface images
&lt;/h3>&lt;h4 id="running-cli-app">&lt;a href="#running-cli-app" class="header-anchor">&lt;/a>running cli app
&lt;/h4>&lt;p>&lt;img src="https://mahanthyalla.in/blogs/DEMO_CLI_OUT_run.png"
loading="lazy"
alt="DEMO_CLI_OUT_run.png"
>&lt;/p>
&lt;h4 id="results---cli">&lt;a href="#results---cli" class="header-anchor">&lt;/a>Results - CLI
&lt;/h4>&lt;p>&lt;img src="https://mahanthyalla.in/blogs/DEMO_CLI_OUT.png"
loading="lazy"
alt="DEMO_CLI_OUT.png"
>&lt;/p>
&lt;h2 id="running-as-a-product-for-an-image-gui">&lt;a href="#running-as-a-product-for-an-image-gui" class="header-anchor">&lt;/a>Running as a product for an image (GUI)
&lt;/h2>&lt;p>Select the image form the drop down and click Run.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">python Main_GUI.py
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="gui-app-interface-images">&lt;a href="#gui-app-interface-images" class="header-anchor">&lt;/a>GUI app interface images
&lt;/h3>&lt;h4 id="running-gui-app">&lt;a href="#running-gui-app" class="header-anchor">&lt;/a>running gui app
&lt;/h4>&lt;p>&lt;img src="https://mahanthyalla.in/blogs/DEMO_GUI_Main_1.png"
loading="lazy"
alt="DEMO_GUI_Main_1.png"
>&lt;/p>
&lt;h4 id="results---gui">&lt;a href="#results---gui" class="header-anchor">&lt;/a>Results - GUI
&lt;/h4>&lt;p>&lt;img src="https://mahanthyalla.in/blogs/DEMO_GUI_Main_2_upload.png"
loading="lazy"
alt="DEMO_GUI_Main_2_upload.png"
>&lt;/p>
&lt;p>&lt;img src="https://mahanthyalla.in/blogs/DEMO_GUI_Main_3_result.png"
loading="lazy"
alt="DEMO_GUI_Main_3_result.png"
>&lt;/p>
&lt;h2 id="build-your-custom-app-using-our-model">&lt;a href="#build-your-custom-app-using-our-model" class="header-anchor">&lt;/a>Build your custom app using our model
&lt;/h2>&lt;p>Yes, you can integrate pothole detection system into any custom app which is already written as an API in the file named as &lt;a class="link" href="https://github.com/Mahanth-Maha/PHD_FYP/blob/main/phd_api.py" target="_blank" rel="noopener"
>phd_api.py&lt;/a>.&lt;/p>
&lt;p>just import as&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">import phd_api
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#create object
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">phd_run = PHD_API()
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">filepath = &amp;#39;test.jpg&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"># use the model
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"># saves output in /Dataset/Result folder
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">phd_run.run_phd_and_save_img(input_image_path=filepath)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"># alternatively use this to not save result
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">phd_run.run_phd(input_image_path=filepath)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="the-results">&lt;a href="#the-results" class="header-anchor">&lt;/a>The Results
&lt;/h3>&lt;p>the Resulting image will be saved in &lt;code>Dataset\Result&lt;/code> folder&lt;/p>
&lt;h2 id="demo">&lt;a href="#demo" class="header-anchor">&lt;/a>Demo
&lt;/h2>&lt;p>Result for an image&lt;/p>
&lt;h3 id="image">&lt;a href="#image" class="header-anchor">&lt;/a>Image
&lt;/h3>&lt;p>&lt;img src="https://mahanthyalla.in/blogs/img00000.JPEG"
loading="lazy"
alt="Original"
>&lt;/p>
&lt;h3 id="result">&lt;a href="#result" class="header-anchor">&lt;/a>Result
&lt;/h3>&lt;p>&lt;img src="https://mahanthyalla.in/blogs/res_img00000.JPEG"
loading="lazy"
alt="detected"
>&lt;/p></description></item><item><title>Zero-Shot Class Unlearning in Deep Computer Vision Models</title><link>https://mahanthyalla.in/blogs/p/zero-shot-unlearning/</link><pubDate>Mon, 05 May 2025 00:00:00 +0000</pubDate><guid>https://mahanthyalla.in/blogs/p/zero-shot-unlearning/</guid><description>&lt;h1 id="zero-shot-class-unlearning-in-deep-computer-vision-models">&lt;a href="#zero-shot-class-unlearning-in-deep-computer-vision-models" class="header-anchor">&lt;/a>Zero-Shot Class Unlearning in Deep Computer Vision Models
&lt;/h1>&lt;p>Forgetting the Past: Targeted Unlearning in Pretrained Deep Networks for Computer Vision - Project Work for Course : DS265 Deep Learning for Computer Vision (DLCV 2025), IISc Bangalore&lt;/p>
&lt;h2 id="project-overview">&lt;a href="#project-overview" class="header-anchor">&lt;/a>Project Overview
&lt;/h2>&lt;p>This project explores and implements methods for &lt;strong>Machine Unlearning&lt;/strong>, specifically focusing on &lt;strong>Zero-Shot Class Unlearning&lt;/strong> in deep learning models for computer vision tasks. The goal is to enable a pre-trained model to &amp;ldquo;forget&amp;rdquo; specific classes it was originally trained on, without requiring access to the original training data (the &amp;ldquo;zero-shot&amp;rdquo; constraint). This is increasingly important due to data privacy regulations (like GDPR&amp;rsquo;s &amp;ldquo;Right to be Forgotten&amp;rdquo;) and the need to remove outdated or sensitive information from deployed models efficiently.&lt;/p>
&lt;p>The core approach leverages &lt;strong>class impressions&lt;/strong> generated directly from the trained model&amp;rsquo;s parameters. These impressions act as data-free proxies for the classes to be forgotten or retained. An unlearning algorithm, inspired by gradient ascent/descent techniques (like NegGrad+) but adapted for the zero-shot setting using only these impressions, modifies the model&amp;rsquo;s weights to suppress information related to the &amp;ldquo;forget&amp;rdquo; classes while preserving knowledge of the &amp;ldquo;retain&amp;rdquo; classes.&lt;/p>
&lt;p>This repository contains implementations for:&lt;/p>
&lt;ul>
&lt;li>Training baseline models (LeNet5, KarpathyNet, AlexNet, ResNet) on relevant datasets (MNIST, CIFAR-10, ImageNet).&lt;/li>
&lt;li>Training &amp;ldquo;Golden Standard&amp;rdquo; models (retrained from scratch without the forget classes) for comparison.&lt;/li>
&lt;li>Performing zero-shot class unlearning using class impressions derived from the trained models.&lt;/li>
&lt;li>Evaluating the effectiveness of unlearning through accuracy metrics and class-wise comparisons.&lt;/li>
&lt;/ul>
&lt;h2 id="project-structure">&lt;a href="#project-structure" class="header-anchor">&lt;/a>Project Structure
&lt;/h2>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-go" data-lang="go">&lt;span class="line">&lt;span class="cl">&lt;span class="nx">unlearning_project&lt;/span>&lt;span class="o">/&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">├──&lt;/span> &lt;span class="nx">data&lt;/span>&lt;span class="o">/&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">│&lt;/span> &lt;span class="err">├──&lt;/span> &lt;span class="nx">MNIST&lt;/span>&lt;span class="o">/&lt;/span> &lt;span class="err">#&lt;/span> &lt;span class="nx">MNIST&lt;/span> &lt;span class="nx">dataset&lt;/span> &lt;span class="nf">files&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="nx">idx&lt;/span> &lt;span class="nx">format&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">│&lt;/span> &lt;span class="err">│&lt;/span> &lt;span class="err">└──&lt;/span> &lt;span class="o">...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">│&lt;/span> &lt;span class="err">├──&lt;/span> &lt;span class="nx">cifar&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="nx">batches&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="nx">py&lt;/span>&lt;span class="o">/&lt;/span> &lt;span class="err">#&lt;/span> &lt;span class="nx">CIFAR&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">10&lt;/span> &lt;span class="nx">dataset&lt;/span> &lt;span class="nf">files&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="nx">python&lt;/span> &lt;span class="nx">batches&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">│&lt;/span> &lt;span class="err">│&lt;/span> &lt;span class="err">└──&lt;/span> &lt;span class="o">...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">│&lt;/span> &lt;span class="err">└──&lt;/span> &lt;span class="nx">ImageNet&lt;/span>&lt;span class="o">/&lt;/span> &lt;span class="err">#&lt;/span> &lt;span class="nx">ImageNet&lt;/span> &lt;span class="nx">dataset&lt;/span> &lt;span class="nx">files&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">│&lt;/span> &lt;span class="err">│&lt;/span> &lt;span class="err">└──&lt;/span> &lt;span class="o">...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">├──&lt;/span> &lt;span class="nx">Learn_LeNet5_Script&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">py&lt;/span> &lt;span class="err">#&lt;/span> &lt;span class="nx">Example&lt;/span> &lt;span class="nx">training&lt;/span> &lt;span class="nf">script&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="nx">can&lt;/span> &lt;span class="nx">be&lt;/span> &lt;span class="nx">adapted&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">├──&lt;/span> &lt;span class="nx">Learn_KarpathyNet_Script&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">py&lt;/span> &lt;span class="err">#&lt;/span> &lt;span class="nx">Example&lt;/span> &lt;span class="nx">training&lt;/span> &lt;span class="nf">script&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="nx">can&lt;/span> &lt;span class="nx">be&lt;/span> &lt;span class="nx">adapted&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">├──&lt;/span> &lt;span class="nx">Learn_KarpathyNet_Golden_Script&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">py&lt;/span> &lt;span class="err">#&lt;/span> &lt;span class="nx">Example&lt;/span> &lt;span class="nx">golden&lt;/span> &lt;span class="nx">standard&lt;/span> &lt;span class="nx">training&lt;/span> &lt;span class="nx">script&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">├──&lt;/span> &lt;span class="nx">Unlearn_LNet_Script&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">py&lt;/span> &lt;span class="err">#&lt;/span> &lt;span class="nx">Unlearning&lt;/span> &lt;span class="nx">script&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="nx">LeNet5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">├──&lt;/span> &lt;span class="nx">Unlearn_KNet_Script&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">py&lt;/span> &lt;span class="err">#&lt;/span> &lt;span class="nx">Unlearning&lt;/span> &lt;span class="nx">script&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="nx">KarpathyNet&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">├──&lt;/span> &lt;span class="nx">requirements&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">txt&lt;/span> &lt;span class="err">#&lt;/span> &lt;span class="nx">Python&lt;/span> &lt;span class="kn">package&lt;/span> &lt;span class="nx">dependencies&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">└──&lt;/span> &lt;span class="nx">README&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">md&lt;/span> &lt;span class="err">#&lt;/span> &lt;span class="nx">This&lt;/span> &lt;span class="nx">file&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="setup-instructions">&lt;a href="#setup-instructions" class="header-anchor">&lt;/a>Setup Instructions
&lt;/h2>&lt;h3 id="prerequisites">&lt;a href="#prerequisites" class="header-anchor">&lt;/a>Prerequisites
&lt;/h3>&lt;ul>
&lt;li>Git&lt;/li>
&lt;li>Conda (or Miniconda)&lt;/li>
&lt;li>NVIDIA GPU with CUDA drivers (recommended for training larger models)&lt;/li>
&lt;/ul>
&lt;h3 id="1-clone-the-repository">&lt;a href="#1-clone-the-repository" class="header-anchor">&lt;/a>1. Clone the Repository
&lt;/h3>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">git clone https://github.com/Mahanth-Maha/ZeroShotUnlearning
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">cd&lt;/span> ZeroShotUnlearning
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="2-create-conda-environment">&lt;a href="#2-create-conda-environment" class="header-anchor">&lt;/a>2. Create Conda Environment
&lt;/h3>&lt;p>We recommend using Python 3.10 or later. Create a Conda environment using the provided &lt;code>requirements.txt&lt;/code> file.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create the environment named &amp;#39;unlearn_env&amp;#39; (or choose your own)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">conda create --name unlearn_env &lt;span class="nv">python&lt;/span>&lt;span class="o">=&lt;/span>3.10 -y
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Activate the environment&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">conda activate unlearn_env
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Install PyTorch with CUDA support (adjust cuda version if needed)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Check PyTorch website (pytorch.org) for the correct command for your system/CUDA version&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Example for CUDA 11.8:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">conda install pytorch torchvision torchaudio pytorch-cuda&lt;span class="o">=&lt;/span>11.8 -c pytorch -c nvidia
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Install other dependencies&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">pip install -r requirements.txt
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="3-download-and-prepare-datasets">&lt;a href="#3-download-and-prepare-datasets" class="header-anchor">&lt;/a>3. Download and Prepare Datasets
&lt;/h3>&lt;ul>
&lt;li>&lt;strong>MNIST:&lt;/strong> Download the IDX files (train-images-idx3-ubyte, train-labels-idx1-ubyte, t10k-images-idx3-ubyte, t10k-labels-idx1-ubyte) and place them in &lt;code>./data/MNIST/&lt;/code>.&lt;/li>
&lt;li>&lt;strong>CIFAR-10:&lt;/strong> Download the Python version batches (data_batch_1 to 5, test_batch, batches.meta) and place them in &lt;code>./data/cifar-10-batches-py/&lt;/code>.&lt;/li>
&lt;li>&lt;strong>ImageNet :&lt;/strong> Place your &lt;code>val&lt;/code>, and &lt;code>test&lt;/code> image files into &lt;code>./data/ImageNet/&lt;/code>.
&lt;ul>
&lt;li>image net validation set used is from Kaggle, which can be downloaded from python script&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">kagglehub&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">path&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">kagglehub&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dataset_download&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;titericz/imagenet1k-val&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Path to dataset files:&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">path&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>&lt;strong>Note:&lt;/strong> The dataset files are not included in this repository due to size constraints. You can download them from their respective sources:&lt;/p>&lt;/blockquote>
&lt;h2 id="usage">&lt;a href="#usage" class="header-anchor">&lt;/a>Usage
&lt;/h2>&lt;h3 id="1-training-original-models">&lt;a href="#1-training-original-models" class="header-anchor">&lt;/a>1. Training Original Models
&lt;/h3>&lt;ul>
&lt;li>
&lt;p>LNet, KNet these are trained with the scripts &lt;code>Learn_LeNet5_Script.py&lt;/code> and &lt;code>Learn_KarpathyNet_Script.py&lt;/code> respectively. These scripts are designed to train the models on the specified datasets (MNIST, CIFAR-10) and save the trained models in the &lt;code>saved_models/&lt;/code> directory.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>AlexNet and ResNet are pretrained models. You can use the &lt;code>torchvision&lt;/code> library to load these models and fine-tune them on your dataset. The training scripts for these models are not provided in this repository, but you can adapt the existing scripts for your needs.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Unlearning works on any other model as long as you have the model architecture and the dataset ready.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="2-training-golden-standard-models-for-comparison">&lt;a href="#2-training-golden-standard-models-for-comparison" class="header-anchor">&lt;/a>2. Training Golden Standard Models (for Comparison)
&lt;/h3>&lt;p>These models are trained from scratch &lt;em>without&lt;/em> the classes intended for forgetting. This provides a benchmark for the unlearning process.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Example (KarpathyNet without classes {3, 4, 8}):&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>Note:&lt;/strong> The forget classes &lt;code>{3, 4, 8}&lt;/code> are currently &lt;strong>hardcoded&lt;/strong> in &lt;code>Learn_KarpathyNet_Golden_Script.py&lt;/code>. You need to &lt;strong>edit the script&lt;/strong> to change the &lt;code>Target_classes&lt;/code> set if you want to forget different classes.&lt;/li>
&lt;li>Run the specific script:
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">python Learn_KarpathyNet_Golden_Script.py -n v1_golden_forget_348 -e &lt;span class="m">100&lt;/span> &lt;span class="c1"># Use a distinct version name&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;/li>
&lt;li>Adapt or create similar scripts for other models/forget sets as needed. The output directory will be based on the &lt;code>--net_name&lt;/code> and &lt;code>-n&lt;/code>/&lt;code>--version_name&lt;/code> arguments used in the script.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="3-performing-zero-shot-unlearning">&lt;a href="#3-performing-zero-shot-unlearning" class="header-anchor">&lt;/a>3. Performing Zero-Shot Unlearning
&lt;/h3>&lt;p>The unlearning scripts load a pre-trained model, generate or load class impressions, and then apply the zero-shot unlearning algorithm.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Notebooks (&lt;code>.ipynb&lt;/code>):&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Files like &lt;code>Unlearn_LNet.ipynb&lt;/code> and &lt;code>Unlearn_KNet.ipynb&lt;/code> provide an interactive way to step through the unlearning process, generate class impressions, visualize results, and understand the core logic. Run these using Jupyter Notebook or Jupyter Lab within your activated conda environment.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Scripts (&lt;code>.py&lt;/code>):&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Files like &lt;code>Unlearn_LNet_Script.py&lt;/code> and &lt;code>Unlearn_KNet_Script.py&lt;/code> are designed to run the unlearning process non-interactively.&lt;/li>
&lt;li>&lt;strong>Modify the Scripts:&lt;/strong> You&amp;rsquo;ll need to &lt;strong>edit these scripts&lt;/strong> to specify:
&lt;ul>
&lt;li>&lt;code>net_name&lt;/code> and &lt;code>version_name&lt;/code>: To load the correct pre-trained model from &lt;code>saved_models&lt;/code>.&lt;/li>
&lt;li>&lt;code>target_classes&lt;/code>: A set of class indices to forget (e.g., &lt;code>{1, 2, 3, 4}&lt;/code> or &lt;code>{3, 4, 8}&lt;/code>).&lt;/li>
&lt;li>Hyperparameters like &lt;code>NUM_SAMPLES&lt;/code> for impressions, &lt;code>LEARN_RATE&lt;/code> for unlearning, &lt;code>epochs&lt;/code>, etc.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Run the Script:&lt;/strong>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Example for LeNet (after editing the script for desired target classes)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">python Unlearn_LNet_Script.py
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Example for KarpathyNet (after editing the script)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">python Unlearn_KNet_Script.py
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="output-the-scripts-will-typically">&lt;a href="#output-the-scripts-will-typically" class="header-anchor">&lt;/a>&lt;strong>Output:&lt;/strong> The scripts will typically:
&lt;/h3>&lt;ol>
&lt;li>Load the specified pre-trained model.&lt;/li>
&lt;li>Determine the responsive layer (usually the last learnable one before the classifier).&lt;/li>
&lt;li>Generate or load class impressions for all classes for that layer (saving them to &lt;code>unlearn/&amp;lt;model_name&amp;gt;_&amp;lt;version_name&amp;gt;/class_impressions/&lt;/code>).&lt;/li>
&lt;li>Create forget/retain datasets &lt;em>using the impressions&lt;/em>.&lt;/li>
&lt;li>Instantiate the &lt;code>ZeroShotUnlearner&lt;/code>.&lt;/li>
&lt;li>Run the unlearning optimization loop (modifying the model &lt;em>in memory&lt;/em>).&lt;/li>
&lt;li>Evaluate the &lt;em>unlearned&lt;/em> model on the test set (overall, forget classes, retain classes).&lt;/li>
&lt;li>Generate comparison plots (Original vs. Unlearned class-wise accuracy).&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>&lt;strong>Note:&lt;/strong> The provided unlearning scripts primarily focus on executing the unlearning process and evaluating its effect &lt;em>immediately&lt;/em>. They might not explicitly save the &lt;em>state&lt;/em> of the unlearned model to a separate file. The &lt;code>u_model&lt;/code> variable within the script holds the unlearned state. You could modify the scripts to save &lt;code>u_model.state_dict()&lt;/code> if needed.&lt;/li>
&lt;/ul>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>Training:&lt;/strong> The scripts starting with &lt;code>Learn&lt;/code> (&lt;code>.py&lt;/code> and &lt;code>.ipynb&lt;/code>) evaluates on the validation set during training and saves logs/plots. Final evaluation on the test set (using the best model checkpoint) is performed at the end.&lt;/li>
&lt;li>&lt;strong>Unlearning:&lt;/strong> The unlearning scripts starting with &lt;code>Unlearn&lt;/code> (&lt;code>.py&lt;/code> and &lt;code>.ipynb&lt;/code>) perform evaluation after the unlearning process:
&lt;ul>
&lt;li>Prints overall accuracy.&lt;/li>
&lt;li>Prints accuracy specifically on the &lt;em>forget&lt;/em> classes (should be close to 0%).&lt;/li>
&lt;li>Prints accuracy specifically on the &lt;em>retain&lt;/em> classes (should be close to the original model&amp;rsquo;s accuracy on these classes, or the golden standard model&amp;rsquo;s accuracy).&lt;/li>
&lt;li>Generates bar plots comparing class-wise accuracies of the original model and the unlearned model.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="note-">&lt;a href="#note-" class="header-anchor">&lt;/a>Note :
&lt;/h3>&lt;ul>
&lt;li>The unlearning process is computationally intensive and may take time, especially for larger models and datasets. Ensure you have sufficient resources (GPU recommended).&lt;/li>
&lt;/ul>
&lt;h3 id="time-complexity">&lt;a href="#time-complexity" class="header-anchor">&lt;/a>Time complexity
&lt;/h3>&lt;p>class impressions generation: is the most time-consuming part of the unlearning process. but it is done only once for each class. The time complexity of generating class impressions is O(n * m), where n is the number of samples in the dataset and m is the number of classes. This is because we need to compute the class impression for each sample in the dataset for each class.&lt;/p>
&lt;p>After generating the class impressions, it is straightforward to compute the forget and retain datasets, and no matter what the number of classes is, the time complexity of this step is O(n), where n is the number of samples in the dataset, for unlearning. The unlearning process itself is O(k * n), where k is the number of epochs and n is the number of samples in the dataset. This is because we need to compute the gradients for each sample in the dataset for each epoch.&lt;/p></description></item></channel></rss>